# Build, Bin, Boost: Instructor Guide

A guide for using the R&D Portfolio Simulation in teaching.

## Overview

Build, Bin, Boost is an experiential learning simulation where students take the role of Head of R&D at a technology company. Over 8 quarters (2 years of game time), they design selection processes, evaluate projects, build portfolios, and make ongoing management decisions. The simulation takes 45-90 minutes to complete and works well for innovation management, technology strategy, and corporate entrepreneurship courses.

## Learning Outcomes

By the end of the session, students should be able to:

- Explain how selection process design affects which projects get funded
- Evaluate innovation projects across multiple dimensions (technical, market, strategic, team)
- Construct a balanced portfolio mixing different risk levels and strategic objectives
- Make reasoned decisions about when to persist with struggling projects versus cutting losses
- Recognise how committee dynamics and cognitive biases influence R&D decisions
- Reflect on their own decision-making patterns and assumptions

## Session Formats

### Format A: Pre-Class Preparation (Recommended)

**Before class**: Students play the simulation individually (45-90 minutes)

**In class** (60-90 minutes):
1. Quick poll of outcomes (10 min) — Who got promoted? Retained? Fired?
2. Small group discussion (20 min) — Compare experiences using discussion questions
3. Debrief and theory connection (30-40 min) — Link experiences to course concepts
4. Key takeaways (10 min)

This format maximises class time for discussion and ensures all students arrive with shared experience.

### Format B: In-Class Activity

**In class** (2-2.5 hours):
1. Introduction and setup (10 min)
2. Individual or team play (60-75 min)
3. Break (10 min)
4. Structured debrief (30-40 min)
5. Theory connection and takeaways (20 min)

Works well for executive education or when pre-work compliance is uncertain.

### Format C: Multi-Session Deep Dive

**Session 1**: Play simulation (homework or in-class)
**Session 2**: Debrief and theory on project selection and portfolio composition
**Session 3**: Second playthrough with different strategy
**Session 4**: Comparative analysis and advanced concepts

Allows deeper exploration of how different approaches lead to different outcomes.

## Modes of Play

### Individual Play
Each student plays alone, making all decisions independently. Best for:
- Maximising individual reflection
- Exposing personal decision-making biases
- Creating diversity of outcomes for class discussion

### Team Play
Small groups (2-4 students) play together, discussing and debating each decision. Best for:
- Generating richer discussion during play
- Practising collaborative decision-making
- Simulating real committee dynamics within the team

**Facilitation tip**: Assign roles within teams (e.g., one person advocates for risky projects, another for safe bets) to surface tensions.

### Competitive Play
Teams or individuals compete for best portfolio performance. Best for:
- Increasing engagement and stakes
- Creating pressure that mimics real organisational dynamics
- Generating post-game analysis of winning versus losing strategies

**Facilitation tip**: Announce a prize for best outcome, but in debrief explore whether "winning" strategies were lucky or skillful.

## Key Teaching Points

### 1. Process Design Matters
The choice of decision-making structure (solo, committee, jury, random) and evaluation approach (scoring, intuition, staged gates) shapes outcomes before any project is evaluated. Students often underestimate this.

**Discussion prompt**: "Did your process help you make better decisions, or did it create blind spots?"

### 2. Portfolio Balance vs. Conviction
Students must balance diversification across strategic priorities against concentration in high-conviction bets. Both over-diversification and over-concentration can fail.

**Discussion prompt**: "How did you decide how many projects to fund? What drove your mix of safe versus risky?"

### 3. The Termination Problem
Killing projects is psychologically difficult, especially after investment. The simulation rewards well-timed kills but penalises both premature termination and throwing good money after bad.

**Discussion prompt**: "Which project was hardest to kill? Why? What signals did you use?"

### 4. Committee Dynamics
The mini-game illustrates how group dynamics, advocacy, and facilitation affect outcomes. Different committee compositions surface different concerns.

**Discussion prompt**: "How did your committee composition affect which projects got attention? What perspectives were missing?"

### 5. Uncertainty and Hindsight
Project outcomes involve randomness—good decisions can fail, poor decisions can succeed. Students should evaluate their decision quality, not just outcomes.

**Discussion prompt**: "Which decisions are you proud of regardless of outcome? Which would you change?"

## Common Student Patterns

| Pattern | What happens | Teaching opportunity |
|---------|--------------|---------------------|
| Playing it safe | Funds only low-risk incremental projects | Board penalises lack of ambition—discuss risk appetite |
| Betting big | Funds only ambitious moonshots | High variance outcomes—discuss portfolio theory |
| Sunk cost trap | Keeps funding failing projects | Discuss escalation of commitment |
| Premature killing | Cuts projects at first bad signal | Discuss patience with different project types |
| Ignoring signals | Continues regardless of updates | Discuss active portfolio management |
| Committee echo chamber | Selects similar-minded committee | Discuss diversity of perspective |

## Connecting to Theory

The simulation connects to several bodies of research and frameworks:

- **Stage-gate processes** (Cooper) — Evaluation checkpoints and go/kill decisions
- **Portfolio management** (Wheelwright & Clark) — Balancing project types and resource allocation
- **Real options** — Value of flexibility and staged investment
- **Escalation of commitment** (Staw) — Why failing projects persist
- **Group decision-making** — Groupthink, advocacy, and devil's advocates
- **Cognitive biases** — Sunk cost fallacy, confirmation bias, overconfidence
- **Ambidexterity** (Tushman & O'Reilly) — Balancing exploitation and exploration

## Assessment Options

### Reflective Report (Individual)
Students submit a 1,000-1,500 word reflection analysing their decisions, outcomes, and lessons learned. See Exercise 4 in the Exercises document.

### Comparative Analysis (Team)
Teams play twice with deliberately different strategies, then submit analysis comparing approaches and outcomes.

### Portfolio Pitch (Presentation)
Students present their portfolio strategy and defend their choices to the class acting as the board.

### Decision Audit
Students document their reasoning for 3-5 key decisions during play, then evaluate quality post-game.

## Troubleshooting

**"I got a bad outcome but made good decisions"**
This is a feature, not a bug. Discuss the difference between decision quality and outcome quality. Innovation involves irreducible uncertainty.

**"The game is unfair / random"**
Projects have different underlying success probabilities based on their characteristics, but outcomes still involve variance. Discuss how real R&D portfolios face similar uncertainty.

**"I didn't have enough information"**
Real R&D decisions are made under uncertainty. The simulation provides substantial information (milestones, teams, TRL, IP, competitors) but not certainty. Discuss information-seeking versus analysis paralysis.

**"My committee gave bad advice"**
Committee members have biases reflecting their roles. The CTO sees technical promise; the CFO sees financial risk. Discuss how to weigh conflicting expert input.

## Technical Notes

- The simulation runs entirely in-browser with no installation required
- Progress is not saved between sessions—students should complete in one sitting
- Works on desktop and tablet; not optimised for phone
- No login or data collection—student results are private unless they choose to share

## Additional Resources

- Student Handout — One-page overview for distribution
- Class Exercises — Structured activities for individual, team, and competitive play
- The simulation includes an in-game tutorial accessible from the landing page

## Background Reading

Academic research on R&D portfolio management, project selection, and innovation decision-making that informed this simulation:

- Brasil, V.C. and Eggers, J.P. (2019). Product and innovation portfolio management. *Oxford Research Encyclopedia of Business and Management*.
- Brasil, V.C., Salerno, M.S., Eggers, J.P. and Gomes, L.A.V. (2021). Boosting radical innovation using ambidextrous portfolio management. *Research-Technology Management*, 64(5), 39-49.
- Boudreau, K.J., Guinan, E.C., Lakhani, K.R. and Riedl, C. (2016). Looking across and looking beyond the knowledge frontier: Intellectual distance, novelty, and resource allocation in science. *Management Science*, 62(10), 2765-2783.
- Cooper, R.G. and Sommer, A.F. (2023). Dynamic portfolio management for new product development. *Research-Technology Management*, 66(3), 19-31.
- Criscuolo, P., Dahlander, L., Grohsjean, T. and Salter, A. (2017). Evaluating novelty: The role of panels in the selection of R&D projects. *Academy of Management Journal*, 60(2), 433-460.
- Criscuolo, P., Dahlander, L., Grohsjean, T. and Salter, A. (2017). The biases that keep good R&D projects from getting funded. *Harvard Business Review*, March 17.
- Criscuolo, P., Dahlander, L., Grohsjean, T. and Salter, A. (2021). The sequence effect on the selection of R&D projects. *Organization Science*, 32(4), 1046-1067.
- Dahlander, L., Beretta, M., Thomas, A., Kazemi, S., Fenger, M.H. and Frederiksen, L. (2023). Weeding out or picking winners in open innovation? Factors driving multi-stage crowd selection on LEGO ideas. *Research Policy*, 52(10), 104875.
- Dahlander, L., Thomas, A., Wallin, M.W. and Ångström, R.C. (2023). Blinded by the person? Experimental evidence from idea evaluation. *Strategic Management Journal*, 44(10), 2443-2459.
- Kumar, A. and Operti, E. (2023). Missed chances and unfulfilled hopes: Why do firms make errors in evaluating technological opportunities? *Strategic Management Journal*, 44(13), 3067-3097.
- Masucci, M., Parker, S.C., Brusoni, S. and Camerani, R. (2021). How are corporate ventures evaluated and selected? *Technovation*, 99, 102126.
- Mount, M.P., Baer, M. and Lupoli, M.J. (2021). Quantum leaps or baby steps? Expertise distance, construal level, and the propensity to invest in novel technological ideas. *Strategic Management Journal*, 42(8), 1490-1515.
- Sharapov, D. and Dahlander, L. (2025). Selection regimes and selection errors. *Organization Science*.
- Si, H., Kavadias, S. and Loch, C. (2022). Managing innovation portfolios: From project selection to portfolio design. *Production and Operations Management*, 31(12), 4572-4588.
- Wilden, R., Lin, N., Hohberger, J. and Randhawa, K. (2023). Selecting innovation projects: Do middle and senior managers differ when it comes to radical innovation? *Journal of Management Studies*, 60(7), 1720-1751.

## Citation

If using this simulation in teaching or research, please cite:

```
Salter, A. (2025). Build, Bin, Boost: The R&D Portfolio Simulation. 
Warwick Business School, The University of Warwick.
https://github.com/ammonhaggerty/build-bin-boost
```

---

*This guide was developed with assistance from Claude (Anthropic).*
